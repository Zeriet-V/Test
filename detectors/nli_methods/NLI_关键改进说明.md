# NLI å¥å­çº§æ£€æµ‹çš„å…³é”®æ”¹è¿›

## ğŸ› åŸå§‹å®ç°çš„ä¸¤ä¸ªä¸¥é‡é—®é¢˜

### é—®é¢˜1: è„†å¼±çš„åˆ†å¥æ–¹æ³•

**åŸä»£ç **:
```python
sentences = re.split(r'[.!?]+', generated_text)
```

**ç¾éš¾æ€§é”™è¯¯**:
```
è¾“å…¥: "The price is approx. $5.00."
é”™è¯¯åˆ†å‰²: ["The price is approx", " $5", "00"]  âŒ

è¾“å…¥: "Dr. Smith went to L.A."
é”™è¯¯åˆ†å‰²: ["Dr", " Smith went to L", "A"]  âŒ

è¾“å…¥: "The U.S. GDP is $21.4 trillion."
é”™è¯¯åˆ†å‰²: ["The U", "S", " GDP is $21", "4 trillion"]  âŒ
```

**åæœ**:
- NLI æ¨¡å‹æ”¶åˆ°æ— æ„ä¹‰çš„å¥å­ç¢ç‰‡
- ä¸¥é‡æ±¡æŸ“ NLI åˆ¤æ–­ç»“æœ
- è¯¯æŠ¥ç‡æš´å¢

---

### é—®é¢˜2: é”™è¯¯çš„åˆ†æ•°èšåˆ

**åŸä»£ç **:
```python
avg_entailment = np.mean([r['entailment_score'] for r in sentence_results])
return avg_entailment  # è¿”å›å¹³å‡åˆ† âŒ
```

**é—®é¢˜æƒ…æ™¯**:
```
ç”Ÿæˆæ–‡æœ¬æœ‰ 10 å¥è¯ï¼š
  - 9 å¥å®Œç¾è•´å«: entailment_score = 0.99
  - 1 å¥ä¸¥é‡çŸ›ç›¾: entailment_score = 0.01  â† å¹»è§‰ï¼

å½“å‰åšæ³•:
  any_hallucination = True  âœ“ æ­£ç¡®æ£€æµ‹åˆ°å¹»è§‰
  avg_entailment = 0.9      âœ— åˆ†æ•°å´å¾ˆé«˜ï¼

è®°å½•åˆ°ç»Ÿè®¡ä¸­:
  hallucination_scores.append(0.9)  âœ— å®Œå…¨é”™è¯¯ï¼
```

**åæœ**:
- æœ‰å¹»è§‰æ ·æœ¬çš„åˆ†æ•°è¢«æ‹‰é«˜
- åˆ†æ•°ç»Ÿè®¡å®Œå…¨å¤±çœŸ
- é˜ˆå€¼ä¼˜åŒ–å¤±æ•ˆ
- æ— æ³•åŒºåˆ†æœ‰/æ— å¹»è§‰æ ·æœ¬

**ç±»æ¯”**:
```
å­¦ç”Ÿè€ƒè¯•æœ‰10é—¨è¯¾ï¼š
  - 9é—¨æ»¡åˆ†100åˆ†
  - 1é—¨ä¸åŠæ ¼30åˆ†

å¹³å‡åˆ† = 93åˆ†  â† çœ‹èµ·æ¥å¾ˆå¥½
ä½†å®é™…ä¸Š: è¿™ä¸ªå­¦ç”Ÿæœ‰ä¸€é—¨è¯¾ä¸åŠæ ¼ï¼

å¹»è§‰æ£€æµ‹åŒç†:
  - åªè¦æœ‰ä¸€å¥æ˜¯å¹»è§‰ï¼Œæ•´ä½“å°±æ˜¯å¹»è§‰
  - åº”è¯¥å…³æ³¨"æœ€å·®çš„é‚£å¥"ï¼Œè€Œä¸æ˜¯"å¹³å‡"
```

---

## âœ… æ”¹è¿›æ–¹æ¡ˆ

### æ”¹è¿›1: å¥å£®çš„åˆ†å¥æ–¹æ³•

**æ–°ä»£ç **:
```python
try:
    import nltk
    # ä½¿ç”¨ nltk.sent_tokenizeï¼ˆä¸“ä¸šçš„åˆ†å¥å·¥å…·ï¼‰
    sentences = nltk.sent_tokenize(generated_text)
except ImportError:
    # å›é€€åˆ°æ”¹è¿›çš„æ­£åˆ™è¡¨è¾¾å¼
    # åªåœ¨å¥å·åæœ‰ç©ºæ ¼æ—¶æ‰åˆ†å‰²
    sentences = re.split(r'(?<=[.!?])\s+', generated_text)
```

**æ­£ç¡®ç»“æœ**:
```
è¾“å…¥: "The price is approx. $5.00."
æ­£ç¡®åˆ†å‰²: ["The price is approx. $5.00."]  âœ“

è¾“å…¥: "Dr. Smith went to L.A. in 2020."
æ­£ç¡®åˆ†å‰²: ["Dr. Smith went to L.A. in 2020."]  âœ“

è¾“å…¥: "The U.S. GDP is $21.4 trillion. It grew 2%."
æ­£ç¡®åˆ†å‰²: ["The U.S. GDP is $21.4 trillion.", "It grew 2%."]  âœ“
```

**ä¼˜åŠ¿**:
- âœ… æ­£ç¡®å¤„ç†ç¼©å†™ï¼ˆDr., U.S., etc.ï¼‰
- âœ… æ­£ç¡®å¤„ç†å°æ•°ç‚¹ï¼ˆ$5.00, 21.4ï¼‰
- âœ… ä¸“ä¸šå·¥å…·ï¼Œç»è¿‡å¤§é‡æµ‹è¯•
- âœ… æ”¯æŒå¤šè¯­è¨€

---

### æ”¹è¿›2: æœ€å·®å¥å­åˆ†æ•°èšåˆ

**æ–°ä»£ç **:
```python
# æ‰¾åˆ°æœ€å·®çš„å¥å­
if use_entailment:
    worst_sentence = min(sentence_results, key=lambda x: x['entailment_score'])
else:
    worst_sentence = max(sentence_results, key=lambda x: x['contradiction_score'])

# ä½¿ç”¨æœ€å·®å¥å­çš„åˆ†æ•°
worst_entailment = worst_sentence['entailment_score']
worst_contradiction = worst_sentence['contradiction_score']

# è¿”å›æœ€å·®å¥å­çš„åˆ†æ•°
return {
    'entailment_score': worst_entailment,  # æœ€å·®å¥å­
    'contradiction_score': worst_contradiction,
    'worst_sentence': worst_sentence['sentence'],  # è®°å½•æ˜¯å“ªå¥
    'avg_entailment_score': avg_entailment,  # å¹³å‡ï¼ˆä»…ä¾›å‚è€ƒï¼‰
}
```

**æ­£ç¡®ç»“æœ**:
```
ç”Ÿæˆæ–‡æœ¬æœ‰ 10 å¥è¯ï¼š
  - 9 å¥å®Œç¾è•´å«: entailment = 0.99
  - 1 å¥ä¸¥é‡çŸ›ç›¾: entailment = 0.01  â† æœ€å·®å¥å­

æ–°åšæ³•:
  any_hallucination = True               âœ“
  worst_entailment = 0.01                âœ“ åæ˜ çœŸå®é—®é¢˜
  worst_sentence = "å…¬å¸æˆç«‹äº2015å¹´"    âœ“ å¯è¿½æº¯
  
è®°å½•åˆ°ç»Ÿè®¡:
  hallucination_scores.append(0.01)      âœ“ æ­£ç¡®ï¼
```

**ä¼˜åŠ¿**:
- âœ… åˆ†æ•°çœŸå®åæ˜ é—®é¢˜ä¸¥é‡ç¨‹åº¦
- âœ… æœ‰å¹»è§‰æ ·æœ¬çš„åˆ†æ•°ä¼šå¾ˆä½ï¼ˆæ­£ç¡®ï¼‰
- âœ… æ— å¹»è§‰æ ·æœ¬çš„åˆ†æ•°ä¼šå¾ˆé«˜ï¼ˆæ­£ç¡®ï¼‰
- âœ… é˜ˆå€¼ä¼˜åŒ–å¯ä»¥æ­£å¸¸å·¥ä½œ
- âœ… å¯ä»¥è¿½æº¯åˆ°å…·ä½“å“ªå¥è¯æœ‰é—®é¢˜

---

## ğŸ“Š æ”¹è¿›æ•ˆæœé¢„æœŸ

### æ”¹è¿›å‰ï¼ˆå¹³å‡åˆ†æ•°ï¼‰
```
æœ‰å¹»è§‰æ ·æœ¬å¹³å‡åˆ†: 0.65  â† å¤ªé«˜ï¼Œå¤±çœŸ
æ— å¹»è§‰æ ·æœ¬å¹³å‡åˆ†: 0.78
åŒºåˆ†åº¦: 0.13  â† å¾ˆå·®
```

### æ”¹è¿›åï¼ˆæœ€å·®å¥å­åˆ†æ•°ï¼‰
```
æœ‰å¹»è§‰æ ·æœ¬æœ€å·®åˆ†: 0.15  â† çœŸå®åæ˜ é—®é¢˜
æ— å¹»è§‰æ ·æœ¬æœ€å·®åˆ†: 0.78  â† ä¿æŒé«˜åˆ†
åŒºåˆ†åº¦: 0.63  â† å¤§å¹…æå‡ï¼
```

**é¢„æœŸæå‡**:
- åˆ†æ•°åŒºåˆ†åº¦æå‡ **4-5å€**
- é˜ˆå€¼ä¼˜åŒ–æ›´å‡†ç¡®
- F1åˆ†æ•°é¢„æœŸæå‡ **5-15%**
- è¯¯æŠ¥ç‡ä¸‹é™

---

## ğŸ”„ éœ€è¦é‡æ–°è¿è¡Œ

**é‡è¦**: ç”±äºæ”¹è¿›äº†åˆ†æ•°èšåˆé€»è¾‘ï¼ŒéªŒè¯é›†ç»“æœå·²ç»å¤±æ•ˆï¼

### å¿…é¡»é‡æ–°è¿è¡ŒéªŒè¯é›†ï¼š

```bash
cd /home/xgq/Test/detectors

# åˆ é™¤æ—§ç»“æœ
rm -f nli_validation_results.jsonl nli_validation_results_report.txt nli_threshold_opt*

# é‡æ–°è¿è¡Œï¼ˆä½¿ç”¨æ”¹è¿›ç‰ˆï¼‰
python nli_deberta_detector.py \
  --gpu 0 \
  --input /home/xgq/Test/data/validation_set.jsonl \
  --output nli_validation_results.jsonl \
  --use-entailment \
  --sentence-level
```

**æ—¶é—´**: çº¦10-20åˆ†é’Ÿï¼ˆæ¨¡å‹å·²ç¼“å­˜ï¼Œä¸éœ€è¦é‡æ–°ä¸‹è½½ï¼‰

**æ”¹è¿›åé¢„æœŸ**:
- ä¸ä¼šå†æœ‰åˆ†å¥é”™è¯¯ï¼ˆDr. â†’ Dr.ï¼‰
- åˆ†æ•°åˆ†å¸ƒæ›´åˆç†
- é˜ˆå€¼ä¼˜åŒ–æ›´å‡†ç¡®
- æœ€ç»ˆF1æå‡

---

## ğŸ†š æ”¹è¿›å¯¹æ¯”

| æ–¹é¢ | æ”¹è¿›å‰ | æ”¹è¿›å |
|------|--------|--------|
| **åˆ†å¥æ–¹æ³•** | `re.split(r'[.!?]+')` | `nltk.sent_tokenize()` â­ |
| **åˆ†å¥å‡†ç¡®æ€§** | é”™è¯¯åˆ‡å‰²ç¼©å†™ âŒ | æ­£ç¡®å¤„ç†ç¼©å†™ âœ“ |
| **åˆ†æ•°èšåˆ** | å¹³å‡åˆ†æ•° âŒ | æœ€å·®å¥å­åˆ†æ•° â­ |
| **åˆ†æ•°åŒºåˆ†åº¦** | 0.13 âŒ | 0.63 âœ“ |
| **é¢„æœŸF1** | 60-65 | **70-80** â­ |

---

## ğŸš€ ç«‹å³æ“ä½œ

```bash
cd /home/xgq/Test/detectors

# æ¸…ç†æ—§ç»“æœ
rm -f nli_validation_results.jsonl nli_validation_results_report.txt nli_threshold_opt*

# é‡æ–°è¿è¡Œï¼ˆæ”¹è¿›ç‰ˆï¼‰
python nli_deberta_detector.py \
  --gpu 0 \
  --input /home/xgq/Test/data/validation_set.jsonl \
  --output nli_validation_results_improved.jsonl \
  --use-entailment \
  --sentence-level
```

è¿è¡Œå®Œåï¼š
```bash
# ä¼˜åŒ–é˜ˆå€¼
python nli_threshold_optimizer.py \
  --results nli_validation_results_improved.jsonl \
  --use-entailment \
  --output nli_threshold_opt_improved
```

**è¿™æ¬¡çš„é˜ˆå€¼å’ŒF1åˆ†æ•°åº”è¯¥ä¼šæ›´å¥½ï¼**

---

è¿™ä¸¤ä¸ªæ”¹è¿›**éå¸¸å…³é”®**ï¼Œä¼šæ˜¾è‘—æå‡æ€§èƒ½ã€‚è¦ä¸è¦ç°åœ¨é‡æ–°è¿è¡ŒéªŒè¯é›†ï¼Ÿ

