"""
DeBERTa-NLI å¹»è§‰æ£€æµ‹å™¨
ä½¿ç”¨è‡ªç„¶è¯­è¨€æ¨ç†(NLI)æ¨¡å‹æ£€æµ‹ç”Ÿæˆæ–‡æœ¬ä¸­çš„å¹»è§‰

åŸç†ï¼š
- å°†åŸæ–‡ä½œä¸º premiseï¼ˆå‰æï¼‰
- å°†ç”Ÿæˆæ–‡æœ¬ä½œä¸º hypothesisï¼ˆå‡è®¾ï¼‰
- NLIæ¨¡å‹åˆ¤æ–­å…³ç³»ï¼šentailmentï¼ˆè•´å«ï¼‰ã€neutralï¼ˆä¸­ç«‹ï¼‰ã€contradictionï¼ˆçŸ›ç›¾ï¼‰
- contradiction è¡¨ç¤ºæœ‰å¹»è§‰ï¼Œentailment è¡¨ç¤ºä¸€è‡´

ä¼˜åŠ¿ï¼š
- ç›´æ¥æ£€æµ‹çŸ›ç›¾å’Œä¸ä¸€è‡´
- DeBERTa åœ¨ NLI ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚
- å¯è§£é‡Šæ€§å¼º
"""

import torch
import json
import os
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import numpy as np

# è®¾ç½®é•œåƒ
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '300'

print(f"ğŸ”§ é•œåƒè®¾ç½®: {os.environ.get('HF_ENDPOINT')}")


class DeBERTaNLIDetector:
    """
    åŸºäº DeBERTa çš„ NLI å¹»è§‰æ£€æµ‹å™¨
    """
    
    def __init__(self, 
                 model_name='MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli',
                 device='cuda' if torch.cuda.is_available() else 'cpu',
                 gpu_id=None):
        """
        åˆå§‹åŒ– NLI æ£€æµ‹å™¨
        
        :param model_name: æ¨¡å‹åç§°ï¼ˆæ¨èï¼‰
            - MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli (æ¨èï¼Œå¤šæ•°æ®é›†è®­ç»ƒï¼Œçº¦1.5GB)
            - microsoft/deberta-large-mnli (æ ‡å‡†ï¼Œçº¦1.4GB)
            - microsoft/deberta-base-mnli (è¾ƒå°ï¼Œçº¦400MB)
            - cross-encoder/nli-deberta-v3-large (DeBERTa-v3)
        :param device: è®¾å¤‡
        :param gpu_id: GPU ID
        """
        if gpu_id is not None and torch.cuda.is_available():
            device = f'cuda:{gpu_id}'
            torch.cuda.set_device(gpu_id)
            print(f"æŒ‡å®šä½¿ç”¨GPU: {gpu_id}")
        
        print(f"åŠ è½½ DeBERTa-NLI æ¨¡å‹: {model_name}")
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        self.device = device
        self.model_name = model_name
        
        # æ£€æŸ¥æœ¬åœ°ç¼“å­˜
        cache_dir = os.path.expanduser('~/.cache/huggingface/hub')
        model_cache = os.path.join(cache_dir, f'models--{model_name.replace("/", "--")}')
        
        try:
            if os.path.exists(model_cache):
                print("æ£€æµ‹åˆ°æœ¬åœ°ç¼“å­˜ï¼Œå°è¯•ç¦»çº¿åŠ è½½...")
                self.tokenizer = AutoTokenizer.from_pretrained(
                    model_name,
                    local_files_only=True
                )
                self.model = AutoModelForSequenceClassification.from_pretrained(
                    model_name,
                    local_files_only=True
                )
                print("âœ“ ç¦»çº¿åŠ è½½æˆåŠŸï¼")
            else:
                print("æœ¬åœ°æ— ç¼“å­˜ï¼Œå¼€å§‹åœ¨çº¿ä¸‹è½½...")
                print("ï¼ˆé¦–æ¬¡ä¸‹è½½çº¦1.3GBï¼Œéœ€è¦å‡ åˆ†é’Ÿï¼‰")
                self.tokenizer = AutoTokenizer.from_pretrained(model_name)
                self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
                print("âœ“ åœ¨çº¿ä¸‹è½½å¹¶åŠ è½½æˆåŠŸï¼")
        except Exception as e:
            print(f"âš  ç¦»çº¿åŠ è½½å¤±è´¥: {str(e)[:100]}")
            print("å°è¯•åœ¨çº¿ä¸‹è½½...")
            import shutil
            if os.path.exists(model_cache):
                shutil.rmtree(model_cache, ignore_errors=True)
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
            print("âœ“ åœ¨çº¿ä¸‹è½½å¹¶åŠ è½½æˆåŠŸï¼")
        
        self.model.eval()
        self.model.to(self.device)
        
        # æ ‡ç­¾æ˜ å°„ï¼ˆDeBERTa-MNLI çš„æ ‡ç­¾é¡ºåºï¼‰
        # 0: contradiction, 1: neutral, 2: entailment
        self.label_mapping = {
            0: 'contradiction',  # çŸ›ç›¾
            1: 'neutral',        # ä¸­ç«‹
            2: 'entailment'      # è•´å«
        }
        
        print("DeBERTa-NLI æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    
    def predict(self, premise, hypothesis):
        """
        é¢„æµ‹ NLI å…³ç³»
        
        :param premise: å‰æï¼ˆåŸæ–‡ï¼‰
        :param hypothesis: å‡è®¾ï¼ˆç”Ÿæˆæ–‡æœ¬ï¼‰
        :return: {'label': str, 'scores': dict, 'contradiction_score': float}
        """
        # Tokenize
        inputs = self.tokenizer(
            premise,
            hypothesis,
            return_tensors='pt',
            truncation=True,
            max_length=512,
            padding=True
        )
        
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits
            probs = torch.softmax(logits, dim=-1)[0]
        
        # è·å–é¢„æµ‹æ ‡ç­¾
        pred_label_id = torch.argmax(probs).item()
        pred_label = self.label_mapping[pred_label_id]
        
        # å„ç±»åˆ«çš„æ¦‚ç‡
        scores = {
            'contradiction': probs[0].item(),
            'neutral': probs[1].item(),
            'entailment': probs[2].item()
        }
        
        return {
            'label': pred_label,
            'scores': scores,
            'contradiction_score': scores['contradiction'],
            'entailment_score': scores['entailment']
        }
    
    def detect_hallucination(self, source_text, generated_text, threshold=0.5, 
                            use_entailment=True, sentence_level=False):
        """
        æ£€æµ‹å¹»è§‰
        
        :param source_text: åŸæ–‡
        :param generated_text: ç”Ÿæˆæ–‡æœ¬
        :param threshold: é˜ˆå€¼
        :param use_entailment: True=ä½¿ç”¨è•´å«åˆ†æ•°(æ¨è), False=ä½¿ç”¨çŸ›ç›¾åˆ†æ•°
        :param sentence_level: æ˜¯å¦è¿›è¡Œå¥å­çº§æ£€æµ‹
        :return: bool, dict
        """
        if sentence_level:
            return self._detect_sentence_level(source_text, generated_text, threshold, use_entailment)
        
        result = self.predict(source_text, generated_text)
        
        # åˆ¤æ–­æ˜¯å¦æœ‰å¹»è§‰
        if use_entailment:
            # ä¿®æ­£A: è•´å«åˆ†æ•°ä¸å¤Ÿé«˜ = æœ‰å¹»è§‰
            has_hallucination = result['entailment_score'] < threshold
        else:
            # åŸæ–¹æ³•: çŸ›ç›¾åˆ†æ•°é«˜ = æœ‰å¹»è§‰
            has_hallucination = result['contradiction_score'] > threshold
        
        return has_hallucination, result
    
    def _detect_sentence_level(self, source_text, generated_text, threshold, use_entailment):
        """
        å¥å­çº§æ£€æµ‹ï¼ˆä¿®æ­£Bï¼‰
        
        :return: bool, dict with sentence-level results
        """
        # æ”¹è¿›1: ä½¿ç”¨æ›´å¥å£®çš„åˆ†å¥æ–¹æ³•
        # ä¼˜å…ˆçº§: SpaCy > NLTK > æ­£åˆ™è¡¨è¾¾å¼
        sentences = None
        
        # æ–¹æ³•1: SpaCyï¼ˆæœ€å‡†ç¡®ï¼‰
        try:
            import spacy
            if not hasattr(self, '_spacy_nlp'):
                # åªåŠ è½½ä¸€æ¬¡ï¼Œç¼“å­˜èµ·æ¥
                try:
                    # åªåŠ è½½åˆ†å¥å™¨ï¼Œç¦ç”¨å…¶ä»–ç»„ä»¶ä»¥æé€Ÿ
                    self._spacy_nlp = spacy.load("en_core_web_sm", disable=["ner", "lemmatizer", "textcat"])
                    # ç¡®ä¿æœ‰åˆ†å¥åŠŸèƒ½
                    if 'sentencizer' not in self._spacy_nlp.pipe_names and 'parser' not in self._spacy_nlp.pipe_names:
                        self._spacy_nlp.add_pipe('sentencizer')
                except OSError:
                    self._spacy_nlp = None
            
            if self._spacy_nlp:
                doc = self._spacy_nlp(generated_text)
                sentences = [sent.text.strip() for sent in doc.sents]
        except Exception:
            # ä»»ä½•SpaCyé”™è¯¯éƒ½è·³è¿‡
            pass
        
        # æ–¹æ³•2: NLTKï¼ˆå¤‡ç”¨ï¼‰
        if sentences is None:
            try:
                import nltk
                try:
                    sentences = nltk.sent_tokenize(generated_text)
                except LookupError:
                    # è·³è¿‡ punkt ä¸‹è½½ï¼Œç›´æ¥ç”¨æ­£åˆ™
                    sentences = None
            except ImportError:
                pass
        
        # æ–¹æ³•3: æ”¹è¿›çš„æ­£åˆ™è¡¨è¾¾å¼ï¼ˆæœ€åå¤‡ç”¨ï¼‰
        if sentences is None:
            import re
            sentences = re.split(r'(?<=[.!?])\s+', generated_text)
            sentences = [s.strip() for s in sentences if s.strip()]
        
        if not sentences:
            # å¦‚æœæ²¡æœ‰å¥å­ï¼Œå›é€€åˆ°æ•´ä½“æ£€æµ‹
            return self.detect_hallucination(source_text, generated_text, threshold, use_entailment, False)
        
        sentence_results = []
        any_hallucination = False
        
        for sent in sentences:
            result = self.predict(source_text, sent)
            
            if use_entailment:
                is_hallucination = result['entailment_score'] < threshold
            else:
                is_hallucination = result['contradiction_score'] > threshold
            
            sentence_results.append({
                'sentence': sent,
                'label': result['label'],
                'entailment_score': result['entailment_score'],
                'contradiction_score': result['contradiction_score'],
                'is_hallucination': is_hallucination
            })
            
            if is_hallucination:
                any_hallucination = True
        
        # æ”¹è¿›2: ä½¿ç”¨"æœ€å·®å¥å­åˆ†æ•°"è€Œé"å¹³å‡åˆ†æ•°"
        # æ‰¾åˆ°æœ€å·®çš„é‚£ä¸ªå¥å­ï¼ˆentailmentåˆ†æ•°æœ€ä½ï¼Œæˆ–contradictionåˆ†æ•°æœ€é«˜ï¼‰
        if use_entailment:
            # æ‰¾entailmentåˆ†æ•°æœ€ä½çš„å¥å­ï¼ˆæœ€å¯èƒ½æ˜¯å¹»è§‰çš„ï¼‰
            worst_sentence = min(sentence_results, key=lambda x: x['entailment_score'])
            worst_entailment = worst_sentence['entailment_score']
            worst_contradiction = worst_sentence['contradiction_score']
        else:
            # æ‰¾contradictionåˆ†æ•°æœ€é«˜çš„å¥å­
            worst_sentence = max(sentence_results, key=lambda x: x['contradiction_score'])
            worst_entailment = worst_sentence['entailment_score']
            worst_contradiction = worst_sentence['contradiction_score']
        
        # è®¡ç®— neutral åˆ†æ•°
        worst_neutral = 1.0 - worst_entailment - worst_contradiction
        
        # åŒæ—¶ä¹Ÿä¿ç•™å¹³å‡åˆ†æ•°ï¼ˆç”¨äºç»Ÿè®¡åˆ†æï¼‰
        avg_entailment = np.mean([r['entailment_score'] for r in sentence_results])
        avg_contradiction = np.mean([r['contradiction_score'] for r in sentence_results])
        avg_neutral = 1.0 - avg_entailment - avg_contradiction
        
        aggregated_result = {
            'label': 'hallucination_detected' if any_hallucination else 'entailment',
            'scores': {
                'contradiction': worst_contradiction,  # ä½¿ç”¨æœ€å·®å¥å­çš„åˆ†æ•°
                'neutral': worst_neutral,
                'entailment': worst_entailment,
            },
            'entailment_score': worst_entailment,  # æœ€å·®å¥å­çš„è•´å«åˆ†æ•°
            'contradiction_score': worst_contradiction,  # æœ€å·®å¥å­çš„çŸ›ç›¾åˆ†æ•°
            'avg_entailment_score': avg_entailment,  # å¹³å‡åˆ†æ•°ï¼ˆä»…ä¾›å‚è€ƒï¼‰
            'avg_contradiction_score': avg_contradiction,
            'worst_sentence': worst_sentence['sentence'],  # æœ€å·®çš„å¥å­å†…å®¹
            'sentence_results': sentence_results,
            'num_sentences': len(sentences),
            'num_hallucination_sentences': sum(r['is_hallucination'] for r in sentence_results)
        }
        
        return any_hallucination, aggregated_result


def process_dataset_nli(input_file='../data/test_response_label.jsonl',
                        output_file='nli_deberta_results.jsonl',
                        model_name='MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli',
                        threshold=0.5,
                        use_entailment=True,
                        sentence_level=False,
                        gpu_id=None):
    """
    ä½¿ç”¨ DeBERTa-NLI æ£€æµ‹æ•°æ®é›†ä¸­çš„å¹»è§‰
    
    :param input_file: è¾“å…¥æ–‡ä»¶
    :param output_file: è¾“å‡ºæ–‡ä»¶
    :param model_name: æ¨¡å‹åç§°
    :param threshold: çŸ›ç›¾åˆ†æ•°é˜ˆå€¼
    :param gpu_id: GPU ID
    """
    print(f"\nã€DeBERTa-NLI å¹»è§‰æ£€æµ‹å™¨ã€‘å¼€å§‹å¤„ç†æ•°æ®é›†: {input_file}")
    print("=" * 80)
    print(f"æ¨¡å‹: {model_name}")
    if use_entailment:
        print(f"è•´å«é˜ˆå€¼: {threshold} (entailment_score < {threshold} åˆ¤å®šä¸ºå¹»è§‰) âœ“ æ¨è")
    else:
        print(f"çŸ›ç›¾é˜ˆå€¼: {threshold} (contradiction_score > {threshold} åˆ¤å®šä¸ºå¹»è§‰)")
    print(f"å¥å­çº§æ£€æµ‹: {'å¼€å¯' if sentence_level else 'å…³é—­'}")
    if gpu_id is not None:
        print(f"æŒ‡å®šGPU: {gpu_id}")
    print("=" * 80)
    
    # åˆå§‹åŒ–æ£€æµ‹å™¨
    detector = DeBERTaNLIDetector(model_name=model_name, gpu_id=gpu_id)
    
    # ç»Ÿè®¡æ•°æ®
    total_count = 0
    has_hallucination_count = 0
    no_hallucination_count = 0
    detected_count = 0
    
    # æ€§èƒ½ç»Ÿè®¡
    stats = {
        'has_label_detected': 0,
        'has_label_not_detected': 0,
        'no_label_detected': 0,
        'no_label_not_detected': 0
    }
    
    # æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡
    task_stats = {
        'Summary': {'total': 0, 'has_label': 0, 'detected': 0, 'true_positive': 0, 'false_negative': 0, 'false_positive': 0, 'contradiction_scores': []},
        'QA': {'total': 0, 'has_label': 0, 'detected': 0, 'true_positive': 0, 'false_negative': 0, 'false_positive': 0, 'contradiction_scores': []},
        'Data2txt': {'total': 0, 'has_label': 0, 'detected': 0, 'true_positive': 0, 'false_negative': 0, 'false_positive': 0, 'contradiction_scores': []}
    }
    
    # æŒ‰å¹»è§‰æ ‡ç­¾ç±»å‹ç»Ÿè®¡
    label_stats = {
        'Evident Conflict': {'total': 0, 'detected': 0, 'samples': []},
        'Subtle Conflict': {'total': 0, 'detected': 0, 'samples': []},
        'Evident Baseless Info': {'total': 0, 'detected': 0, 'samples': []},
        'Subtle Baseless Info': {'total': 0, 'detected': 0, 'samples': []}
    }
    
    # åˆ†æ•°ç»Ÿè®¡
    all_contradiction_scores = []
    hallucination_contradiction_scores = []
    no_hallucination_contradiction_scores = []
    
    # è¯¯åˆ¤æ ·æœ¬
    false_positive_samples = []
    false_negative_samples = []
    
    fp_by_task = {'Summary': 0, 'QA': 0, 'Data2txt': 0}
    fn_by_task = {'Summary': 0, 'QA': 0, 'Data2txt': 0}
    fn_by_label = {
        'Evident Conflict': 0,
        'Subtle Conflict': 0,
        'Evident Baseless Info': 0,
        'Subtle Baseless Info': 0
    }
    
    with open(input_file, 'r', encoding='utf-8') as fin, \
         open(output_file, 'w', encoding='utf-8') as fout:
        
        lines = fin.readlines()
        
        for line in tqdm(lines, desc="NLIæ£€æµ‹"):
            total_count += 1
            data = json.loads(line)
            
            # è§£ææ•°æ®
            test_data = data.get('test', '')
            task_type = data.get('task_type', 'Unknown')
            
            # å¤„ç†ä¸åŒä»»åŠ¡ç±»å‹
            if isinstance(test_data, dict):
                if task_type == 'QA':
                    question = test_data.get('question', '')
                    passages = test_data.get('passages', '')
                    text_original = f"{question} {passages}"
                elif task_type == 'Data2txt':
                    parts = []
                    if 'name' in test_data: parts.append(f"Name: {test_data['name']}")
                    if 'address' in test_data: parts.append(f"Address: {test_data['address']}")
                    if 'city' in test_data and 'state' in test_data: 
                        parts.append(f"Location: {test_data['city']}, {test_data['state']}")
                    if 'categories' in test_data: parts.append(f"Categories: {test_data['categories']}")
                    if 'business_stars' in test_data: parts.append(f"Rating: {test_data['business_stars']} stars")
                    if 'review_info' in test_data and isinstance(test_data['review_info'], list):
                        for i, review in enumerate(test_data['review_info'][:3], 1):
                            if isinstance(review, dict) and 'review_text' in review:
                                parts.append(f"Review {i}: {review['review_text']}")
                    text_original = ' '.join(parts)
                else:
                    text_original = str(test_data)
            else:
                text_original = test_data
            
            text_generated = data.get('response', '')
            label_types = data.get('label_types', [])
            
            if not isinstance(text_original, str): text_original = str(text_original)
            if not isinstance(text_generated, str): text_generated = str(text_generated)
            
            if not text_original.strip() or not text_generated.strip():
                continue
            
            has_label = len(label_types) > 0
            if has_label:
                has_hallucination_count += 1
            else:
                no_hallucination_count += 1
            
            try:
                # NLI æ£€æµ‹
                detected, nli_result = detector.detect_hallucination(
                    text_original, 
                    text_generated, 
                    threshold,
                    use_entailment=use_entailment,
                    sentence_level=sentence_level
                )
                
                if detected:
                    detected_count += 1
                
                contradiction_score = nli_result['contradiction_score']
                
                # è®°å½•åˆ†æ•°
                all_contradiction_scores.append(contradiction_score)
                if has_label:
                    hallucination_contradiction_scores.append(contradiction_score)
                else:
                    no_hallucination_contradiction_scores.append(contradiction_score)
                
                # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
                if has_label and detected:
                    stats['has_label_detected'] += 1
                elif has_label and not detected:
                    stats['has_label_not_detected'] += 1
                    false_negative_samples.append({
                        'id': data.get('id', ''),
                        'task_type': task_type,
                        'label_types': label_types,
                        'contradiction_score': float(contradiction_score),
                        'predicted_label': nli_result['label']
                    })
                    if task_type in fn_by_task:
                        fn_by_task[task_type] += 1
                    for label_type in label_types:
                        if label_type in fn_by_label:
                            fn_by_label[label_type] += 1
                elif not has_label and detected:
                    stats['no_label_detected'] += 1
                    false_positive_samples.append({
                        'id': data.get('id', ''),
                        'task_type': task_type,
                        'contradiction_score': float(contradiction_score),
                        'predicted_label': nli_result['label']
                    })
                    if task_type in fp_by_task:
                        fp_by_task[task_type] += 1
                else:
                    stats['no_label_not_detected'] += 1
                
                # æ›´æ–°ä»»åŠ¡ç±»å‹ç»Ÿè®¡
                if task_type in task_stats:
                    task_stats[task_type]['total'] += 1
                    task_stats[task_type]['contradiction_scores'].append(contradiction_score)
                    if has_label: task_stats[task_type]['has_label'] += 1
                    if detected: task_stats[task_type]['detected'] += 1
                    if has_label and detected: task_stats[task_type]['true_positive'] += 1
                    elif has_label and not detected: task_stats[task_type]['false_negative'] += 1
                    elif not has_label and detected: task_stats[task_type]['false_positive'] += 1
                
                # æ›´æ–°å¹»è§‰æ ‡ç­¾ç±»å‹ç»Ÿè®¡
                for label_type in label_types:
                    if label_type in label_stats:
                        label_stats[label_type]['total'] += 1
                        if detected:
                            label_stats[label_type]['detected'] += 1
                        if len(label_stats[label_type]['samples']) < 5:
                            label_stats[label_type]['samples'].append(data.get('id', ''))
                
                # ä¿å­˜ç»“æœ
                result = {
                    'id': data.get('id', ''),
                    'task_type': task_type,
                    'has_label': has_label,
                    'label_types': label_types,
                    'nli_label': nli_result['label'],
                    'contradiction_score': float(contradiction_score),
                    'entailment_score': float(nli_result['entailment_score']),
                    'neutral_score': float(nli_result['scores'].get('neutral', 0.0)),  # å®‰å…¨è®¿é—®
                    'detected': detected,
                    'threshold': threshold
                }
                
                # å¦‚æœæ˜¯å¥å­çº§æ£€æµ‹ï¼Œæ·»åŠ é¢å¤–ä¿¡æ¯
                if sentence_level and 'sentence_results' in nli_result:
                    result['num_sentences'] = nli_result.get('num_sentences', 0)
                    result['num_hallucination_sentences'] = nli_result.get('num_hallucination_sentences', 0)
                
                fout.write(json.dumps(result, ensure_ascii=False) + '\n')
                
            except Exception as e:
                print(f"\nå¤„ç†é”™è¯¯ (ID: {data.get('id', 'unknown')}): {str(e)}")
                continue
    
    # ============ ç”ŸæˆæŠ¥å‘Š ============
    report_file = output_file.replace('.jsonl', '_report.txt')
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("DeBERTa-NLI å¹»è§‰æ£€æµ‹è¯¦ç»†æŠ¥å‘Š\n")
        f.write("=" * 80 + "\n\n")
        
        f.write("ã€æ£€æµ‹ç­–ç•¥ã€‘\n")
        f.write("  æ–¹æ³•: DeBERTa-NLI - åŸºäºè‡ªç„¶è¯­è¨€æ¨ç†çš„å¹»è§‰æ£€æµ‹\n")
        f.write(f"  æ¨¡å‹: {model_name}\n")
        if use_entailment:
            f.write(f"  æ£€æµ‹é˜ˆå€¼: {threshold} (entailment_score < {threshold} åˆ¤å®šä¸ºå¹»è§‰)\n")
            f.write("  åˆ¤å®šæ ‡å‡†: éè•´å«å³å¹»è§‰ (contradiction + neutral â†’ å¹»è§‰)\n")
        else:
            f.write(f"  æ£€æµ‹é˜ˆå€¼: {threshold} (contradiction_score > {threshold} åˆ¤å®šä¸ºå¹»è§‰)\n")
            f.write("  åˆ¤å®šæ ‡å‡†: çŸ›ç›¾å³å¹»è§‰\n")
        f.write(f"  å¥å­çº§æ£€æµ‹: {'å¼€å¯' if sentence_level else 'å…³é—­'}\n")
        f.write("  åŸç†: å°†åŸæ–‡ä½œä¸ºpremiseï¼Œç”Ÿæˆæ–‡æœ¬ä½œä¸ºhypothesisï¼Œæ£€æµ‹é€»è¾‘å…³ç³»\n\n")
        
        f.write("ã€æ€»ä½“ç»Ÿè®¡ã€‘\n")
        f.write(f"  æ€»æ•°æ®é‡: {total_count}\n")
        f.write(f"  - æœ‰æ ‡ç­¾ï¼ˆæœ‰å¹»è§‰ï¼‰: {has_hallucination_count} ({has_hallucination_count/total_count*100:.2f}%)\n")
        f.write(f"  - æ— æ ‡ç­¾ï¼ˆæ— å¹»è§‰ï¼‰: {no_hallucination_count} ({no_hallucination_count/total_count*100:.2f}%)\n")
        f.write(f"  - æ£€æµ‹åˆ°å¹»è§‰: {detected_count} ({detected_count/total_count*100:.2f}%)\n\n")
        
        f.write("ã€çŸ›ç›¾åˆ†æ•°åˆ†æã€‘\n")
        f.write(f"  å…¨éƒ¨æ ·æœ¬:\n")
        f.write(f"    å¹³å‡çŸ›ç›¾åˆ†æ•°: {np.mean(all_contradiction_scores):.4f}\n")
        f.write(f"    æ ‡å‡†å·®: {np.std(all_contradiction_scores):.4f}\n")
        f.write(f"    æœ€å°å€¼: {np.min(all_contradiction_scores):.4f}\n")
        f.write(f"    æœ€å¤§å€¼: {np.max(all_contradiction_scores):.4f}\n\n")
        
        if hallucination_contradiction_scores:
            f.write(f"  æœ‰å¹»è§‰æ ·æœ¬:\n")
            f.write(f"    å¹³å‡çŸ›ç›¾åˆ†æ•°: {np.mean(hallucination_contradiction_scores):.4f}\n")
            f.write(f"    æ ‡å‡†å·®: {np.std(hallucination_contradiction_scores):.4f}\n\n")
        
        if no_hallucination_contradiction_scores:
            f.write(f"  æ— å¹»è§‰æ ·æœ¬:\n")
            f.write(f"    å¹³å‡çŸ›ç›¾åˆ†æ•°: {np.mean(no_hallucination_contradiction_scores):.4f}\n")
            f.write(f"    æ ‡å‡†å·®: {np.std(no_hallucination_contradiction_scores):.4f}\n\n")
        
        # æ€§èƒ½æŒ‡æ ‡
        tp = stats['has_label_detected']
        fp = stats['no_label_detected']
        fn = stats['has_label_not_detected']
        tn = stats['no_label_not_detected']
        
        precision = tp / (tp + fp) * 100 if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) * 100 if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        f.write("ã€æ•´ä½“æ€§èƒ½æŒ‡æ ‡ã€‘\n")
        f.write(f"  âœ“ çœŸé˜³æ€§ (TP): {tp}\n")
        f.write(f"  âœ— å‡é˜´æ€§ (FN): {fn}\n")
        f.write(f"  âœ— å‡é˜³æ€§ (FP): {fp}\n")
        f.write(f"  âœ“ çœŸé˜´æ€§ (TN): {tn}\n\n")
        f.write(f"  å‡†ç¡®ç‡ (Precision): {precision:.2f}%\n")
        f.write(f"  å¬å›ç‡ (Recall): {recall:.2f}%\n")
        f.write(f"  F1åˆ†æ•°: {f1:.2f}\n\n")
        
        # æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡
        f.write("=" * 80 + "\n")
        f.write("ã€æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡ã€‘\n")
        f.write("=" * 80 + "\n\n")
        
        for task, stats_data in task_stats.items():
            if stats_data['total'] > 0:
                task_recall = stats_data['true_positive'] / stats_data['has_label'] * 100 if stats_data['has_label'] > 0 else 0
                task_precision = stats_data['true_positive'] / stats_data['detected'] * 100 if stats_data['detected'] > 0 else 0
                task_f1 = 2 * task_precision * task_recall / (task_precision + task_recall) if (task_precision + task_recall) > 0 else 0
                avg_score = np.mean(stats_data['contradiction_scores']) if stats_data['contradiction_scores'] else 0
                
                f.write(f"â—† {task} ä»»åŠ¡:\n")
                f.write(f"  æ€»æ•°: {stats_data['total']}\n")
                f.write(f"  å¹³å‡çŸ›ç›¾åˆ†æ•°: {avg_score:.4f}\n")
                f.write(f"  - æœ‰å¹»è§‰æ•°æ®: {stats_data['has_label']} ({stats_data['has_label']/stats_data['total']*100:.2f}%)\n")
                f.write(f"  - æ£€æµ‹åˆ°å¹»è§‰: {stats_data['detected']} ({stats_data['detected']/stats_data['total']*100:.2f}%)\n\n")
                f.write(f"  æ€§èƒ½è¡¨ç°:\n")
                f.write(f"    âœ“ æˆåŠŸæ£€æµ‹ (TP): {stats_data['true_positive']}\n")
                f.write(f"    âœ— æ¼æ£€ (FN): {stats_data['false_negative']}\n")
                f.write(f"    âœ— è¯¯æŠ¥ (FP): {stats_data['false_positive']}\n")
                f.write(f"    å‡†ç¡®ç‡: {task_precision:.2f}%\n")
                f.write(f"    å¬å›ç‡: {task_recall:.2f}%\n")
                f.write(f"    F1åˆ†æ•°: {task_f1:.2f}\n\n")
        
        # æŒ‰å¹»è§‰ç±»å‹ç»Ÿè®¡
        f.write("=" * 80 + "\n")
        f.write("ã€æŒ‰å¹»è§‰ç±»å‹ç»Ÿè®¡ã€‘\n")
        f.write("=" * 80 + "\n\n")
        
        for label_type, label_data in label_stats.items():
            if label_data['total'] > 0:
                detection_rate = label_data['detected'] / label_data['total'] * 100
                miss_count = label_data['total'] - label_data['detected']
                miss_rate = 100 - detection_rate
                
                f.write(f"â—† {label_type}:\n")
                f.write(f"  æ€»æ•°: {label_data['total']}\n")
                f.write(f"  æ£€æµ‹åˆ°: {label_data['detected']} ({detection_rate:.2f}%)\n")
                f.write(f"  æ¼æ£€: {miss_count} ({miss_rate:.2f}%)\n")
                f.write(f"  çŠ¶æ€: {'âœ“ æ£€æµ‹æ•ˆæœå¥½' if detection_rate >= 80 else 'âœ— éœ€è¦æ”¹è¿›'}\n")
                f.write(f"  æ ·æœ¬ID (å‰5ä¸ª): {label_data['samples']}\n\n")
        
        f.write("=" * 80 + "\n")
        f.write(f"ç»“æœå·²ä¿å­˜åˆ°: {output_file}\n")
        f.write("=" * 80 + "\n")
    
    print(f"\nè¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    
    # æ‰“å°æ€§èƒ½æ‘˜è¦
    print("\n" + "=" * 80)
    print("ã€DeBERTa-NLI æ£€æµ‹æ€§èƒ½æ‘˜è¦ã€‘")
    print("=" * 80)
    print(f"çŸ›ç›¾é˜ˆå€¼: {threshold}")
    print(f"\nå‡†ç¡®ç‡ (Precision): {precision:.2f}%")
    print(f"å¬å›ç‡ (Recall): {recall:.2f}%")
    print(f"F1åˆ†æ•°: {f1:.2f}")
    print(f"\nçŸ›ç›¾åˆ†æ•°ç»Ÿè®¡:")
    print(f"  å¹³å‡åˆ†æ•°: {np.mean(all_contradiction_scores):.4f}")
    print(f"  æœ‰å¹»è§‰: {np.mean(hallucination_contradiction_scores):.4f}")
    print(f"  æ— å¹»è§‰: {np.mean(no_hallucination_contradiction_scores):.4f}")
    print("=" * 80)


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='DeBERTa-NLI å¹»è§‰æ£€æµ‹å™¨')
    parser.add_argument('--gpu', type=int, default=None, help='æŒ‡å®šGPU ID')
    parser.add_argument('--input', type=str, default='../data/test_response_label.jsonl', help='è¾“å…¥æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str, default='nli_deberta_results.jsonl', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--threshold', type=float, default=0.5, help='é˜ˆå€¼ï¼ˆé»˜è®¤0.5ï¼‰')
    parser.add_argument('--use-entailment', action='store_true', default=True, 
                        help='ä½¿ç”¨è•´å«åˆ†æ•°åˆ¤å®šï¼ˆæ¨èï¼Œé»˜è®¤å¼€å¯ï¼‰')
    parser.add_argument('--use-contradiction', dest='use_entailment', action='store_false',
                        help='ä½¿ç”¨çŸ›ç›¾åˆ†æ•°åˆ¤å®šï¼ˆä¸æ¨èï¼‰')
    parser.add_argument('--sentence-level', action='store_true', 
                        help='å¯ç”¨å¥å­çº§æ£€æµ‹ï¼ˆæ¨èï¼‰')
    parser.add_argument('--model', type=str, 
                        default='MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli', 
                        help='æ¨¡å‹åç§° (é»˜è®¤: MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli)')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥GPU
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        print(f"\næ£€æµ‹åˆ° {gpu_count} å¼ GPUå¡:")
        for i in range(gpu_count):
            print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
        
        if args.gpu is not None:
            if args.gpu >= gpu_count:
                print(f"âš  è­¦å‘Š: GPU {args.gpu} ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨GPU 0")
                args.gpu = 0
    else:
        print("âš  è­¦å‘Š: æœªæ£€æµ‹åˆ°GPU")
        args.gpu = None
    
    # è¿è¡Œæ£€æµ‹
    process_dataset_nli(
        input_file=args.input,
        output_file=args.output,
        model_name=args.model,
        threshold=args.threshold,
        use_entailment=args.use_entailment,
        sentence_level=args.sentence_level,
        gpu_id=args.gpu
    )

