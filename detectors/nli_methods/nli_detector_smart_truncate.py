"""
NLI æ£€æµ‹å™¨ - æ™ºèƒ½æˆªæ–­ç‰ˆæœ¬
è§£å†³è¾“å…¥æ–‡æœ¬è¿‡é•¿å¯¼è‡´çš„å‡†ç¡®ç‡ä½é—®é¢˜

æ ¸å¿ƒæ”¹è¿›:
1. æ™ºèƒ½æˆªå–åŸæ–‡å…³é”®éƒ¨åˆ†ï¼ˆè€Œéå…¨éƒ¨ï¼‰
2. å¯¹ä¸åŒä»»åŠ¡ä½¿ç”¨ä¸åŒæˆªå–ç­–ç•¥
3. ç¡®ä¿è¾“å…¥åœ¨ NLI æ¨¡å‹çš„æœ‰æ•ˆèŒƒå›´å†…
"""

import torch
import json
import os
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import numpy as np
import re

os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '300'

print(f"ğŸ”§ é•œåƒè®¾ç½®: {os.environ.get('HF_ENDPOINT')}")

from transformers import AutoTokenizer, AutoModelForSequenceClassification


def smart_truncate_source(source_text, generated_text, task_type, max_length=400):
    """
    æ™ºèƒ½æˆªå–åŸæ–‡
    
    ç­–ç•¥:
    - Summary: ä¿ç•™åŸæ–‡å¼€å¤´å’Œä¸ç”Ÿæˆæ–‡æœ¬ç›¸å…³çš„éƒ¨åˆ†
    - QA: ä¼˜å…ˆä¿ç•™questionå’Œç›¸å…³passageç‰‡æ®µ
    - Data2txt: ä¿ç•™æœ€ç›¸å…³çš„å­—æ®µ
    
    :param source_text: åŸæ–‡
    :param generated_text: ç”Ÿæˆæ–‡æœ¬
    :param task_type: ä»»åŠ¡ç±»å‹
    :param max_length: æœ€å¤§å­—ç¬¦æ•°
    :return: æˆªå–åçš„åŸæ–‡
    """
    if len(source_text) <= max_length:
        return source_text
    
    # ç­–ç•¥1: ä¿ç•™å¼€å¤´ + ä¸ç”Ÿæˆæ–‡æœ¬ç›¸å…³çš„éƒ¨åˆ†
    # ç®€å•å®ç°: å–å‰ max_length å­—ç¬¦
    # é«˜çº§å®ç°: å¯ä»¥ç”¨ TF-IDF æ‰¾æœ€ç›¸å…³çš„å¥å­
    
    if task_type == 'Summary':
        # æ‘˜è¦ä»»åŠ¡: ä¿ç•™åŸæ–‡å¼€å¤´éƒ¨åˆ†
        return source_text[:max_length]
    
    elif task_type == 'QA':
        # QAä»»åŠ¡: å°è¯•ä¿ç•™ question å’Œéƒ¨åˆ† passage
        # å‡è®¾æ ¼å¼æ˜¯ "question passage"
        parts = source_text.split(maxsplit=1)
        if len(parts) == 2:
            question, passage = parts
            # ä¿ç•™å®Œæ•´question + éƒ¨åˆ†passage
            if len(question) < max_length:
                remaining = max_length - len(question) - 1
                return question + " " + passage[:remaining]
        return source_text[:max_length]
    
    elif task_type == 'Data2txt':
        # Data2txt: ä¿ç•™å‰é¢çš„å…³é”®å­—æ®µ
        return source_text[:max_length]
    
    else:
        return source_text[:max_length]


class SmartNLIDetector:
    """
    æ™ºèƒ½ NLI æ£€æµ‹å™¨ï¼ˆå¸¦æˆªæ–­ï¼‰
    """
    
    def __init__(self, 
                 model_name='MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli',
                 device='cuda' if torch.cuda.is_available() else 'cpu',
                 gpu_id=None,
                 max_source_length=400):
        """
        :param max_source_length: åŸæ–‡æœ€å¤§é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰
        """
        if gpu_id is not None and torch.cuda.is_available():
            device = f'cuda:{gpu_id}'
            torch.cuda.set_device(gpu_id)
            print(f"æŒ‡å®šä½¿ç”¨GPU: {gpu_id}")
        
        print(f"åŠ è½½ DeBERTa-NLI æ¨¡å‹: {model_name}")
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        print(f"åŸæ–‡æœ€å¤§é•¿åº¦: {max_source_length} å­—ç¬¦")
        
        self.device = device
        self.model_name = model_name
        self.max_source_length = max_source_length
        
        # åŠ è½½æ¨¡å‹
        cache_dir = os.path.expanduser('~/.cache/huggingface/hub')
        model_cache = os.path.join(cache_dir, f'models--{model_name.replace("/", "--")}')
        
        try:
            if os.path.exists(model_cache):
                print("æ£€æµ‹åˆ°æœ¬åœ°ç¼“å­˜ï¼Œå°è¯•ç¦»çº¿åŠ è½½...")
                self.tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=True)
                self.model = AutoModelForSequenceClassification.from_pretrained(model_name, local_files_only=True)
                print("âœ“ ç¦»çº¿åŠ è½½æˆåŠŸï¼")
            else:
                print("æœ¬åœ°æ— ç¼“å­˜ï¼Œå¼€å§‹åœ¨çº¿ä¸‹è½½...")
                self.tokenizer = AutoTokenizer.from_pretrained(model_name)
                self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
                print("âœ“ åœ¨çº¿ä¸‹è½½å¹¶åŠ è½½æˆåŠŸï¼")
        except Exception as e:
            print(f"âš  ç¦»çº¿åŠ è½½å¤±è´¥ï¼Œå°è¯•åœ¨çº¿ä¸‹è½½...")
            import shutil
            if os.path.exists(model_cache):
                shutil.rmtree(model_cache, ignore_errors=True)
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
            print("âœ“ åœ¨çº¿ä¸‹è½½å¹¶åŠ è½½æˆåŠŸï¼")
        
        self.model.eval()
        self.model.to(self.device)
        
        # æ ‡ç­¾æ˜ å°„
        self.label_mapping = {
            0: 'contradiction',
            1: 'neutral',
            2: 'entailment'
        }
        
        print("DeBERTa-NLI æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    
    def predict(self, premise, hypothesis, task_type='Unknown'):
        """
        é¢„æµ‹ NLI å…³ç³»ï¼ˆå¸¦æ™ºèƒ½æˆªæ–­ï¼‰
        """
        # æ™ºèƒ½æˆªå–åŸæ–‡
        premise_truncated = smart_truncate_source(premise, hypothesis, task_type, self.max_source_length)
        
        # Tokenize
        inputs = self.tokenizer(
            premise_truncated,
            hypothesis,
            return_tensors='pt',
            truncation=True,
            max_length=512,
            padding=True
        )
        
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits
            probs = torch.softmax(logits, dim=-1)[0]
        
        pred_label_id = torch.argmax(probs).item()
        pred_label = self.label_mapping[pred_label_id]
        
        scores = {
            'contradiction': probs[0].item(),
            'neutral': probs[1].item(),
            'entailment': probs[2].item()
        }
        
        return {
            'label': pred_label,
            'scores': scores,
            'contradiction_score': scores['contradiction'],
            'entailment_score': scores['entailment'],
            'premise_truncated': len(premise_truncated) < len(premise)
        }
    
    def detect_hallucination(self, source_text, generated_text, task_type, threshold=0.5, use_contradiction=True):
        """
        æ£€æµ‹å¹»è§‰
        
        :param use_contradiction: True=ä½¿ç”¨çŸ›ç›¾åˆ†æ•°, False=ä½¿ç”¨è•´å«åˆ†æ•°
        """
        result = self.predict(source_text, generated_text, task_type)
        
        if use_contradiction:
            has_hallucination = result['contradiction_score'] > threshold
        else:
            has_hallucination = result['entailment_score'] < threshold
        
        return has_hallucination, result


# ç®€åŒ–ç‰ˆå¤„ç†å‡½æ•°ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰
def test_smart_truncate(input_file, output_file, gpu_id=0, max_source_length=400):
    """
    æµ‹è¯•æ™ºèƒ½æˆªæ–­ç‰ˆæœ¬
    """
    print(f"\nã€æ™ºèƒ½æˆªæ–­ NLI æ£€æµ‹å™¨ã€‘")
    print("=" * 80)
    print(f"åŸæ–‡æœ€å¤§é•¿åº¦: {max_source_length} å­—ç¬¦")
    print("=" * 80)
    
    detector = SmartNLIDetector(gpu_id=gpu_id, max_source_length=max_source_length)
    
    # ç»Ÿè®¡
    total = 0
    tp = fp = fn = tn = 0
    truncated_count = 0
    
    with open(input_file, 'r', encoding='utf-8') as fin, \
         open(output_file, 'w', encoding='utf-8') as fout:
        
        lines = fin.readlines()
        
        for line in tqdm(lines[:500], desc="æµ‹è¯•ä¸­ï¼ˆå‰500æ ·æœ¬ï¼‰"):  # å…ˆæµ‹è¯•500ä¸ª
            total += 1
            data = json.loads(line)
            
            # è§£ææ•°æ®
            test_data = data.get('test', '')
            task_type = data.get('task_type', 'Unknown')
            
            # ç®€åŒ–çš„æ–‡æœ¬æå–
            if isinstance(test_data, dict):
                text_original = str(test_data)
            else:
                text_original = test_data
            
            text_generated = data.get('response', '')
            has_label = len(data.get('label_types', [])) > 0
            
            if not text_original or not text_generated:
                continue
            
            try:
                detected, result = detector.detect_hallucination(
                    text_original, text_generated, task_type,
                    threshold=0.5, use_contradiction=True
                )
                
                if result['premise_truncated']:
                    truncated_count += 1
                
                if has_label and detected: tp += 1
                elif has_label and not detected: fn += 1
                elif not has_label and detected: fp += 1
                else: tn += 1
                
                fout.write(json.dumps({
                    'id': data.get('id'),
                    'has_label': has_label,
                    'detected': detected,
                    'contradiction_score': result['contradiction_score'],
                    'truncated': result['premise_truncated']
                }, ensure_ascii=False) + '\n')
                
            except Exception as e:
                print(f"\né”™è¯¯: {str(e)[:100]}")
                continue
    
    # è®¡ç®—æŒ‡æ ‡
    precision = tp / (tp + fp) * 100 if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) * 100 if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    
    print(f"\nç»“æœï¼ˆå‰500æ ·æœ¬ï¼‰:")
    print(f"  å‡†ç¡®ç‡: {precision:.2f}%")
    print(f"  å¬å›ç‡: {recall:.2f}%")
    print(f"  F1åˆ†æ•°: {f1:.2f}")
    print(f"  æˆªæ–­æ ·æœ¬: {truncated_count}/{total} ({truncated_count/total*100:.1f}%)")


if __name__ == "__main__":
    # æµ‹è¯•ä¸åŒçš„æˆªæ–­é•¿åº¦
    for max_len in [200, 400, 800, 1200]:
        print(f"\n\n{'='*80}")
        print(f"æµ‹è¯• max_length = {max_len}")
        print("=" * 80)
        
        test_smart_truncate(
            input_file='/home/xgq/Test/data/validation_set.jsonl',
            output_file=f'test_truncate_{max_len}.jsonl',
            gpu_id=0,
            max_source_length=max_len
        )


