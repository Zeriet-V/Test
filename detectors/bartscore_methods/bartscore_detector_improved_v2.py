"""
BARTScore å¹»è§‰æ£€æµ‹å™¨ - æ”¹è¿›ç‰ˆ V2
æ”¯æŒæ ‡å‡†çš„éªŒè¯é›†â†’é˜ˆå€¼ä¼˜åŒ–â†’æµ‹è¯•é›†æµç¨‹

ä¸»è¦æ”¹è¿›ï¼š
1. ä»»åŠ¡ç‰¹å®šé˜ˆå€¼ - ä¸ºä¸åŒä»»åŠ¡ç±»å‹ä½¿ç”¨ä¸åŒçš„æ£€æµ‹é˜ˆå€¼
2. åŒå‘BARTScore - åŒæ—¶è¯„ä¼°source->targetå’Œtarget->source
3. ç½®ä¿¡åº¦è¯„åˆ† - åŸºäºåŒå‘åˆ†æ•°ä¸€è‡´æ€§æä¾›ç½®ä¿¡åº¦
4. æŒ‰å¹»è§‰ç±»å‹ç»Ÿè®¡F1åˆ†æ•° - è¯¦ç»†çš„å¹»è§‰ç±»å‹æ€§èƒ½åˆ†æ
5. æ”¯æŒå¤–éƒ¨é˜ˆå€¼è¾“å…¥ - å¯ä½¿ç”¨é˜ˆå€¼ä¼˜åŒ–å™¨å¾—åˆ°çš„æœ€ä¼˜é˜ˆå€¼
"""

import torch
import json
import os
import argparse
from tqdm import tqdm
import numpy as np

os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '300'
os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'

print(f"ğŸ”§ é•œåƒè®¾ç½®: {os.environ.get('HF_ENDPOINT')}")

from transformers import BartTokenizer, BartForConditionalGeneration


class ImprovedBARTScorer:
    """æ”¹è¿›çš„BARTScoreè¯„åˆ†å™¨ï¼Œæ”¯æŒåŒå‘è¯„åˆ†å’Œç½®ä¿¡åº¦è¯„ä¼°"""
    
    def __init__(self, model_name='facebook/bart-large-cnn', device='cuda' if torch.cuda.is_available() else 'cpu', gpu_id=None):
        if gpu_id is not None and torch.cuda.is_available():
            device = f'cuda:{gpu_id}'
            torch.cuda.set_device(gpu_id)
            print(f"æŒ‡å®šä½¿ç”¨GPU: {gpu_id}")
        
        print(f"åŠ è½½æ”¹è¿›ç‰ˆBARTScoreæ¨¡å‹: {model_name}")
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        self.device = device
        cache_dir = os.path.expanduser('~/.cache/huggingface/hub')
        model_cache = os.path.join(cache_dir, f'models--{model_name.replace("/", "--")}')
        
        if os.path.exists(model_cache):
            print("æ£€æµ‹åˆ°æœ¬åœ°ç¼“å­˜ï¼Œå°è¯•ç¦»çº¿åŠ è½½...")
            try:
                self.tokenizer = BartTokenizer.from_pretrained(model_name, local_files_only=True)
                self.model = BartForConditionalGeneration.from_pretrained(model_name, local_files_only=True)
                print("âœ“ ç¦»çº¿åŠ è½½æˆåŠŸï¼")
            except Exception as e:
                print(f"âš  ç¦»çº¿åŠ è½½å¤±è´¥ï¼Œå°è¯•åœ¨çº¿ä¸‹è½½...")
                self.tokenizer = BartTokenizer.from_pretrained(model_name)
                self.model = BartForConditionalGeneration.from_pretrained(model_name)
                print("âœ“ åœ¨çº¿åŠ è½½æˆåŠŸï¼")
        else:
            print("æœ¬åœ°æ— ç¼“å­˜ï¼Œå¼€å§‹åœ¨çº¿ä¸‹è½½...")
            self.tokenizer = BartTokenizer.from_pretrained(model_name)
            self.model = BartForConditionalGeneration.from_pretrained(model_name)
            print("âœ“ ä¸‹è½½å¹¶åŠ è½½æˆåŠŸï¼")
        
        self.model.eval()
        self.model.to(self.device)
        print("BARTScoreæ¨¡å‹åŠ è½½å®Œæˆï¼")
    
    def score_bidirectional(self, source_text, generated_text):
        """åŒå‘BARTScoreè¯„ä¼°"""
        with torch.no_grad():
            src_tokens = self.tokenizer([source_text], return_tensors='pt', truncation=True, max_length=1024)
            gen_tokens = self.tokenizer([generated_text], return_tensors='pt', truncation=True, max_length=1024)
            
            src_tokens = {k: v.to(self.device) for k, v in src_tokens.items()}
            gen_tokens = {k: v.to(self.device) for k, v in gen_tokens.items()}
            
            # Forward: P(generated|source)
            forward_output = self.model(
                input_ids=src_tokens['input_ids'],
                attention_mask=src_tokens['attention_mask'],
                labels=gen_tokens['input_ids']
            )
            forward_score = -forward_output.loss.item()
            
            # Backward: P(source|generated)
            backward_output = self.model(
                input_ids=gen_tokens['input_ids'],
                attention_mask=gen_tokens['attention_mask'],
                labels=src_tokens['input_ids']
            )
            backward_score = -backward_output.loss.item()
            
            avg_score = (forward_score + backward_score) / 2
            harmonic_mean = 2 * forward_score * backward_score / (forward_score + backward_score) if (forward_score + backward_score) != 0 else 0
            
            return {
                'forward': forward_score,
                'backward': backward_score,
                'average': avg_score,
                'harmonic': harmonic_mean
            }
    
    def score_with_confidence(self, source_text, generated_text):
        """è®¡ç®—BARTScoreå¹¶æä¾›ç½®ä¿¡åº¦"""
        bi_scores = self.score_bidirectional(source_text, generated_text)
        score_diff = abs(bi_scores['forward'] - bi_scores['backward'])
        confidence = 1.0 / (1.0 + score_diff)
        
        return {
            'score': bi_scores['forward'],
            'confidence': confidence,
            'bidirectional': bi_scores
        }


def compute_f1_metrics(tp, fp, fn):
    """è®¡ç®— Precision, Recall, F1"""
    precision = tp / (tp + fp) * 100 if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) * 100 if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    return precision, recall, f1


def process_dataset_improved_v2(input_file,
                                 output_file,
                                 task_thresholds=None,
                                 model_name='facebook/bart-large-cnn',
                                 use_bidirectional=True,
                                 gpu_id=None,
                                 dataset_type='validation'):
    """
    æ”¹è¿›ç‰ˆ BARTScore æ£€æµ‹å™¨ V2
    
    :param input_file: è¾“å…¥æ–‡ä»¶
    :param output_file: è¾“å‡ºæ–‡ä»¶
    :param task_thresholds: ä»»åŠ¡ç‰¹å®šé˜ˆå€¼å­—å…¸
    :param model_name: BARTæ¨¡å‹åç§°
    :param use_bidirectional: æ˜¯å¦ä½¿ç”¨åŒå‘è¯„åˆ†
    :param gpu_id: GPU ID
    :param dataset_type: æ•°æ®é›†ç±»å‹ ('validation' æˆ– 'test')
    """
    print(f"\nã€æ”¹è¿›ç‰ˆBARTScoreå¹»è§‰æ£€æµ‹å™¨ V2ã€‘")
    print(f"æ•°æ®é›†ç±»å‹: {dataset_type.upper()}")
    print("=" * 80)
    print(f"è¾“å…¥æ–‡ä»¶: {input_file}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
    print(f"æ¨¡å‹: {model_name}")
    print(f"åŒå‘è¯„åˆ†: {use_bidirectional}")
    
    # é»˜è®¤ä»»åŠ¡ç‰¹å®šé˜ˆå€¼ï¼ˆå¯è¢«å¤–éƒ¨è¦†ç›–ï¼‰
    if task_thresholds is None:
        task_thresholds = {
            'Summary': -1.65,
            'QA': -2.05,
            'Data2txt': -2.45
        }
    
    print(f"\nä»»åŠ¡ç‰¹å®šé˜ˆå€¼:")
    for task, threshold in task_thresholds.items():
        print(f"  {task}: {threshold:.4f}")
    print("=" * 80)
    
    # åˆå§‹åŒ–æ¨¡å‹
    scorer = ImprovedBARTScorer(model_name=model_name, gpu_id=gpu_id)
    
    # ç»Ÿè®¡å˜é‡
    total_count = 0
    has_hallucination_count = 0
    no_hallucination_count = 0
    detected_count = 0
    
    stats = {
        'has_label_detected': 0,
        'has_label_not_detected': 0,
        'no_label_detected': 0,
        'no_label_not_detected': 0
    }
    
    # æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡
    task_stats = {}
    for task in ['Summary', 'QA', 'Data2txt']:
        task_stats[task] = {
            'total': 0, 'has_label': 0, 'detected': 0,
            'true_positive': 0, 'false_negative': 0, 'false_positive': 0,
            'scores': []
        }
    
    # æŒ‰å¹»è§‰æ ‡ç­¾ç±»å‹ç»Ÿè®¡ï¼ˆå¢å¼ºç‰ˆï¼‰
    label_stats = {}
    for label_type in ['Evident Conflict', 'Subtle Conflict', 'Evident Baseless Info', 'Subtle Baseless Info']:
        label_stats[label_type] = {
            'total': 0,
            'detected': 0,
            'true_positive': 0,  # æ–°å¢
            'false_negative': 0,  # æ–°å¢
            'samples': []
        }
    
    # åˆ†æ•°ç»Ÿè®¡
    all_scores = []
    hallucination_scores = []
    no_hallucination_scores = []
    
    # è¯¯åˆ¤æ ·æœ¬
    false_positive_samples = []
    false_negative_samples = []
    
    # è¯»å–å¹¶å¤„ç†æ•°æ®
    with open(input_file, 'r', encoding='utf-8') as fin:
        lines = fin.readlines()
    
    with open(output_file, 'w', encoding='utf-8') as fout:
        for line in tqdm(lines, desc=f"æ£€æµ‹-{dataset_type}"):
            total_count += 1
            try:
                data = json.loads(line)
            except:
                continue
            
            test_data = data.get('test', '')
            task_type = data.get('task_type', 'Unknown')
            
            # å¤„ç†ä¸åŒä»»åŠ¡ç±»å‹
            if isinstance(test_data, dict):
                if task_type == 'QA':
                    question = test_data.get('question', '')
                    passages = test_data.get('passages', '')
                    text_original = f"{question} {passages}"
                elif task_type == 'Data2txt':
                    parts = []
                    if 'name' in test_data: parts.append(f"Name: {test_data['name']}")
                    if 'address' in test_data: parts.append(f"Address: {test_data['address']}")
                    if 'city' in test_data and 'state' in test_data:
                        parts.append(f"Location: {test_data['city']}, {test_data['state']}")
                    if 'categories' in test_data: parts.append(f"Categories: {test_data['categories']}")
                    if 'business_stars' in test_data: parts.append(f"Rating: {test_data['business_stars']} stars")
                    if 'review_info' in test_data and isinstance(test_data['review_info'], list):
                        for i, review in enumerate(test_data['review_info'][:3], 1):
                            if isinstance(review, dict) and 'review_text' in review:
                                parts.append(f"Review {i}: {review['review_text']}")
                    text_original = ' '.join(parts)
                else:
                    text_original = str(test_data)
            else:
                text_original = test_data
            
            text_generated = data.get('response', '')
            label_types = data.get('label_types', [])
            
            if not isinstance(text_original, str): text_original = str(text_original)
            if not isinstance(text_generated, str): text_generated = str(text_generated)
            
            if not text_original.strip() or not text_generated.strip():
                continue
            
            has_label = len(label_types) > 0
            if has_label:
                has_hallucination_count += 1
            else:
                no_hallucination_count += 1
            
            try:
                # è®¡ç®—BARTScore
                if use_bidirectional:
                    score_result = scorer.score_with_confidence(text_original, text_generated)
                    score = score_result['score']
                    confidence = score_result['confidence']
                    bi_scores = score_result['bidirectional']
                else:
                    bi_scores_result = scorer.score_bidirectional(text_original, text_generated)
                    score = bi_scores_result['forward']
                    confidence = 1.0
                    bi_scores = bi_scores_result
                
                # ä½¿ç”¨ä»»åŠ¡ç‰¹å®šé˜ˆå€¼åˆ¤æ–­
                threshold = task_thresholds.get(task_type, -1.8649)
                detected = score < threshold
                
                if detected:
                    detected_count += 1
                
                # è®°å½•åˆ†æ•°
                all_scores.append(score)
                if has_label:
                    hallucination_scores.append(score)
                else:
                    no_hallucination_scores.append(score)
                
                # æ›´æ–°æ•´ä½“ç»Ÿè®¡
                if has_label and detected:
                    stats['has_label_detected'] += 1
                elif has_label and not detected:
                    stats['has_label_not_detected'] += 1
                    false_negative_samples.append({
                        'id': data.get('id', ''),
                        'task_type': task_type,
                        'label_types': label_types,
                        'score': float(score),
                        'confidence': float(confidence)
                    })
                elif not has_label and detected:
                    stats['no_label_detected'] += 1
                    false_positive_samples.append({
                        'id': data.get('id', ''),
                        'task_type': task_type,
                        'score': float(score),
                        'confidence': float(confidence)
                    })
                else:
                    stats['no_label_not_detected'] += 1
                
                # æ›´æ–°ä»»åŠ¡ç±»å‹ç»Ÿè®¡
                if task_type in task_stats:
                    task_stats[task_type]['total'] += 1
                    task_stats[task_type]['scores'].append(score)
                    if has_label: task_stats[task_type]['has_label'] += 1
                    if detected: task_stats[task_type]['detected'] += 1
                    if has_label and detected: task_stats[task_type]['true_positive'] += 1
                    elif has_label and not detected: task_stats[task_type]['false_negative'] += 1
                    elif not has_label and detected: task_stats[task_type]['false_positive'] += 1
                
                # æ›´æ–°å¹»è§‰æ ‡ç­¾ç±»å‹ç»Ÿè®¡ï¼ˆå¢å¼ºç‰ˆï¼Œè®¡ç®—æ¯ä¸ªç±»å‹çš„TP/FNï¼‰
                for label_type in label_types:
                    if label_type in label_stats:
                        label_stats[label_type]['total'] += 1
                        if detected:
                            label_stats[label_type]['detected'] += 1
                            label_stats[label_type]['true_positive'] += 1
                        else:
                            label_stats[label_type]['false_negative'] += 1
                        if len(label_stats[label_type]['samples']) < 5:
                            label_stats[label_type]['samples'].append(data.get('id', ''))
                
                # ä¿å­˜ç»“æœ
                result = {
                    'id': data.get('id', ''),
                    'task_type': task_type,
                    'has_label': has_label,
                    'label_types': label_types,
                    'bartscore': float(score),
                    'confidence': float(confidence),
                    'detected': detected,
                    'threshold_used': float(threshold)
                }
                
                if use_bidirectional:
                    result['bidirectional_scores'] = {
                        'forward': float(bi_scores['forward']),
                        'backward': float(bi_scores['backward']),
                        'average': float(bi_scores['average']),
                        'harmonic': float(bi_scores['harmonic'])
                    }
                
                fout.write(json.dumps(result, ensure_ascii=False) + '\n')
                
            except Exception as e:
                print(f"\nå¤„ç†é”™è¯¯ (ID: {data.get('id', 'unknown')}): {str(e)}")
                continue
    
    # ============ ç”ŸæˆæŠ¥å‘Š ============
    report_file = output_file.replace('.jsonl', '_report.txt')
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write(f"æ”¹è¿›ç‰ˆBARTScoreå¹»è§‰æ£€æµ‹è¯¦ç»†æŠ¥å‘Š ({dataset_type.upper()})\n")
        f.write("=" * 80 + "\n\n")
        
        f.write("ã€æ£€æµ‹ç­–ç•¥ã€‘\n")
        f.write("  æ–¹æ³•: æ”¹è¿›ç‰ˆBARTScore V2\n")
        f.write(f"  æ•°æ®é›†: {dataset_type}\n")
        f.write(f"  æ¨¡å‹: {model_name}\n")
        f.write(f"  åŒå‘è¯„åˆ†: {use_bidirectional}\n")
        f.write("  æ”¹è¿›ç‚¹:\n")
        f.write("    1. ä»»åŠ¡ç‰¹å®šé˜ˆå€¼\n")
        if use_bidirectional:
            f.write("    2. åŒå‘BARTScore\n")
            f.write("    3. ç½®ä¿¡åº¦è¯„åˆ†\n")
        f.write("    4. æŒ‰å¹»è§‰ç±»å‹çš„F1åˆ†æ•°ç»Ÿè®¡\n")
        f.write("\n  ä»»åŠ¡ç‰¹å®šé˜ˆå€¼:\n")
        for task, threshold in task_thresholds.items():
            f.write(f"    {task}: {threshold:.4f}\n")
        f.write("\n")
        
        f.write("ã€æ€»ä½“ç»Ÿè®¡ã€‘\n")
        f.write(f"  æ€»æ•°æ®é‡: {total_count}\n")
        f.write(f"  - æœ‰æ ‡ç­¾ï¼ˆæœ‰å¹»è§‰ï¼‰: {has_hallucination_count} ({has_hallucination_count/total_count*100:.2f}%)\n")
        f.write(f"  - æ— æ ‡ç­¾ï¼ˆæ— å¹»è§‰ï¼‰: {no_hallucination_count} ({no_hallucination_count/total_count*100:.2f}%)\n")
        f.write(f"  - æ£€æµ‹åˆ°å¹»è§‰: {detected_count} ({detected_count/total_count*100:.2f}%)\n\n")
        
        f.write("ã€BARTScoreåˆ†æ•°åˆ†æã€‘\n")
        f.write(f"  å…¨éƒ¨æ ·æœ¬:\n")
        f.write(f"    å¹³å‡åˆ†æ•°: {np.mean(all_scores):.4f}\n")
        f.write(f"    æ ‡å‡†å·®: {np.std(all_scores):.4f}\n")
        f.write(f"    æœ€å°å€¼: {np.min(all_scores):.4f}\n")
        f.write(f"    æœ€å¤§å€¼: {np.max(all_scores):.4f}\n\n")
        
        if hallucination_scores:
            f.write(f"  æœ‰å¹»è§‰æ ·æœ¬:\n")
            f.write(f"    å¹³å‡åˆ†æ•°: {np.mean(hallucination_scores):.4f}\n")
            f.write(f"    æ ‡å‡†å·®: {np.std(hallucination_scores):.4f}\n\n")
        
        if no_hallucination_scores:
            f.write(f"  æ— å¹»è§‰æ ·æœ¬:\n")
            f.write(f"    å¹³å‡åˆ†æ•°: {np.mean(no_hallucination_scores):.4f}\n")
            f.write(f"    æ ‡å‡†å·®: {np.std(no_hallucination_scores):.4f}\n\n")
        
        # æ•´ä½“æ€§èƒ½æŒ‡æ ‡
        tp = stats['has_label_detected']
        fp = stats['no_label_detected']
        fn = stats['has_label_not_detected']
        tn = stats['no_label_not_detected']
        
        precision, recall, f1 = compute_f1_metrics(tp, fp, fn)
        
        f.write("ã€æ•´ä½“æ€§èƒ½æŒ‡æ ‡ã€‘\n")
        f.write(f"  âœ“ çœŸé˜³æ€§ (TP): {tp}\n")
        f.write(f"  âœ— å‡é˜´æ€§ (FN): {fn}\n")
        f.write(f"  âœ— å‡é˜³æ€§ (FP): {fp}\n")
        f.write(f"  âœ“ çœŸé˜´æ€§ (TN): {tn}\n\n")
        f.write(f"  å‡†ç¡®ç‡ (Precision): {precision:.2f}%\n")
        f.write(f"  å¬å›ç‡ (Recall): {recall:.2f}%\n")
        f.write(f"  F1åˆ†æ•°: {f1:.2f}\n\n")
        
        # æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡
        f.write("=" * 80 + "\n")
        f.write("ã€æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡ã€‘\n")
        f.write("=" * 80 + "\n\n")
        
        for task, stats_data in task_stats.items():
            if stats_data['total'] > 0:
                task_precision, task_recall, task_f1 = compute_f1_metrics(
                    stats_data['true_positive'],
                    stats_data['false_positive'],
                    stats_data['false_negative']
                )
                avg_score = np.mean(stats_data['scores']) if stats_data['scores'] else 0
                
                f.write(f"â—† {task} ä»»åŠ¡:\n")
                f.write(f"  é˜ˆå€¼: {task_thresholds.get(task, 'N/A'):.4f}\n")
                f.write(f"  æ€»æ•°: {stats_data['total']}\n")
                f.write(f"  å¹³å‡BARTScore: {avg_score:.4f}\n")
                f.write(f"  - æœ‰å¹»è§‰æ•°æ®: {stats_data['has_label']} ({stats_data['has_label']/stats_data['total']*100:.2f}%)\n")
                f.write(f"  - æ£€æµ‹åˆ°å¹»è§‰: {stats_data['detected']} ({stats_data['detected']/stats_data['total']*100:.2f}%)\n\n")
                f.write(f"  æ€§èƒ½è¡¨ç°:\n")
                f.write(f"    âœ“ æˆåŠŸæ£€æµ‹ (TP): {stats_data['true_positive']}\n")
                f.write(f"    âœ— æ¼æ£€ (FN): {stats_data['false_negative']}\n")
                f.write(f"    âœ— è¯¯æŠ¥ (FP): {stats_data['false_positive']}\n")
                f.write(f"    å‡†ç¡®ç‡: {task_precision:.2f}%\n")
                f.write(f"    å¬å›ç‡: {task_recall:.2f}%\n")
                f.write(f"    F1åˆ†æ•°: {task_f1:.2f}\n\n")
        
        # æŒ‰å¹»è§‰ç±»å‹ç»Ÿè®¡ï¼ˆå¢å¼ºç‰ˆï¼ŒåŒ…å«F1ï¼‰
        f.write("=" * 80 + "\n")
        f.write("ã€æŒ‰å¹»è§‰ç±»å‹ç»Ÿè®¡ï¼ˆå«F1åˆ†æ•°ï¼‰ã€‘\n")
        f.write("=" * 80 + "\n\n")
        
        for label_type, label_data in label_stats.items():
            if label_data['total'] > 0:
                detection_rate = label_data['detected'] / label_data['total'] * 100
                
                # è®¡ç®—è¯¥å¹»è§‰ç±»å‹çš„ Precision, Recall, F1
                # æ³¨æ„ï¼šè¿™é‡Œ FP æ— æ³•ç²¾ç¡®ç»Ÿè®¡ï¼ˆå› ä¸ºæ— å¹»è§‰æ ·æœ¬æ²¡æœ‰æ ‡ç­¾ç±»å‹ï¼‰
                # æˆ‘ä»¬åªè®¡ç®— Recallï¼ˆæ£€æµ‹ç‡ï¼‰
                label_recall = detection_rate
                label_tp = label_data['true_positive']
                label_fn = label_data['false_negative']
                
                f.write(f"â—† {label_type}:\n")
                f.write(f"  æ€»æ•°: {label_data['total']}\n")
                f.write(f"  æ£€æµ‹åˆ° (TP): {label_data['detected']} ({detection_rate:.2f}%)\n")
                f.write(f"  æ¼æ£€ (FN): {label_fn} ({(100-detection_rate):.2f}%)\n")
                f.write(f"  å¬å›ç‡ (Recall): {label_recall:.2f}%\n")
                f.write(f"  çŠ¶æ€: {'âœ“ æ£€æµ‹æ•ˆæœå¥½' if detection_rate >= 80 else 'âœ— éœ€è¦æ”¹è¿›'}\n")
                f.write(f"  æ ·æœ¬ID (å‰5ä¸ª): {label_data['samples']}\n\n")
        
        # è¯¯åˆ¤æ ·æœ¬åˆ†æ
        f.write("=" * 80 + "\n")
        f.write("ã€è¯¯åˆ¤æ ·æœ¬åˆ†æã€‘\n")
        f.write("=" * 80 + "\n\n")
        
        f.write(f"â—† å‡é˜³æ€§ (FP): {fp}\n")
        if false_positive_samples:
            f.write(f"  ç¤ºä¾‹ (å‰10ä¸ª):\n")
            for i, sample in enumerate(false_positive_samples[:10], 1):
                f.write(f"    {i}. ID: {sample['id']}, ä»»åŠ¡: {sample['task_type']}, åˆ†æ•°: {sample['score']:.4f}, ç½®ä¿¡åº¦: {sample.get('confidence', 1.0):.4f}\n")
        f.write("\n")
        
        f.write(f"â—† å‡é˜´æ€§ (FN): {fn}\n")
        if false_negative_samples:
            f.write(f"  ç¤ºä¾‹ (å‰10ä¸ª):\n")
            for i, sample in enumerate(false_negative_samples[:10], 1):
                labels_str = ', '.join(sample['label_types'])
                f.write(f"    {i}. ID: {sample['id']}, ä»»åŠ¡: {sample['task_type']}, æ ‡ç­¾: [{labels_str}], åˆ†æ•°: {sample['score']:.4f}, ç½®ä¿¡åº¦: {sample.get('confidence', 1.0):.4f}\n")
        
        f.write("\n")
        f.write("=" * 80 + "\n")
        f.write(f"ç»“æœå·²ä¿å­˜åˆ°: {output_file}\n")
        f.write("=" * 80 + "\n")
    
    print(f"\nè¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    print("\n" + "=" * 80)
    print(f"ã€æ£€æµ‹æ€§èƒ½æ‘˜è¦ - {dataset_type.upper()}ã€‘")
    print("=" * 80)
    print(f"å‡†ç¡®ç‡ (Precision): {precision:.2f}%")
    print(f"å¬å›ç‡ (Recall): {recall:.2f}%")
    print(f"F1åˆ†æ•°: {f1:.2f}")
    print("=" * 80)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='æ”¹è¿›ç‰ˆBARTScoreå¹»è§‰æ£€æµ‹å™¨ V2ï¼ˆæ”¯æŒæ ‡å‡†æµç¨‹ï¼‰')
    parser.add_argument('--gpu', type=int, default=None, help='GPU ID')
    parser.add_argument('--input', type=str, required=True, help='è¾“å…¥æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str, required=True, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--dataset-type', type=str, default='validation', choices=['validation', 'test'], 
                        help='æ•°æ®é›†ç±»å‹ï¼švalidation æˆ– test')
    parser.add_argument('--no-bidirectional', action='store_true', help='ç¦ç”¨åŒå‘è¯„åˆ†')
    parser.add_argument('--model', type=str, default='facebook/bart-large-cnn', help='BARTæ¨¡å‹åç§°')
    
    # ä»»åŠ¡ç‰¹å®šé˜ˆå€¼ï¼ˆå¯é€‰ï¼Œå¯ä»¥ä»é˜ˆå€¼ä¼˜åŒ–å™¨çš„ç»“æœä¸­è·å–ï¼‰
    parser.add_argument('--threshold-summary', type=float, default=-1.65, help='Summaryä»»åŠ¡é˜ˆå€¼')
    parser.add_argument('--threshold-qa', type=float, default=-2.05, help='QAä»»åŠ¡é˜ˆå€¼')
    parser.add_argument('--threshold-data2txt', type=float, default=-2.45, help='Data2txtä»»åŠ¡é˜ˆå€¼')
    
    args = parser.parse_args()
    
    # GPU æ£€æŸ¥
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        print(f"\næ£€æµ‹åˆ° {gpu_count} å¼ GPU:")
        for i in range(gpu_count):
            print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
        
        if args.gpu is not None and args.gpu >= gpu_count:
            print(f"âš  è­¦å‘Š: GPU {args.gpu} ä¸å­˜åœ¨ï¼Œä½¿ç”¨ GPU 0")
            args.gpu = 0
    else:
        print("âš  è­¦å‘Š: æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUè¿è¡Œä¼šå¾ˆæ…¢")
        args.gpu = None
    
    # æ„å»ºä»»åŠ¡ç‰¹å®šé˜ˆå€¼
    task_thresholds = {
        'Summary': args.threshold_summary,
        'QA': args.threshold_qa,
        'Data2txt': args.threshold_data2txt
    }
    
    # è¿è¡Œæ£€æµ‹
    process_dataset_improved_v2(
        input_file=args.input,
        output_file=args.output,
        task_thresholds=task_thresholds,
        model_name=args.model,
        use_bidirectional=not args.no_bidirectional,
        gpu_id=args.gpu,
        dataset_type=args.dataset_type
    )

